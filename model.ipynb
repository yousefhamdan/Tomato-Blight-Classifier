{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import splitfolders\n",
    "\n",
    "splitfolders.ratio('tometo_disease\\\\training\\PlantVillage', output='train_test', ratio=(0.8, 0.2))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN layers (3 convolutional layers with max pooling, 1 dense layer, and a softmax output layer)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=(255, 255, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12804 images belonging to 10 classes.\n",
      "Found 3207 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Using generator to input the training and validation images. Also, used image augmentation on the training data.\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ").flow_from_directory('train_test/train',\n",
    "                      target_size=(255, 255),\n",
    "                      batch_size=256,\n",
    "                      class_mode='categorical')\n",
    "\n",
    "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    ").flow_from_directory('train_test/val',\n",
    "                      target_size=(255, 255),\n",
    "                      batch_size=64,\n",
    "                      class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51/51 [==============================] - 312s 6s/step - loss: 1.9999 - accuracy: 0.2665 - val_loss: 1.8216 - val_accuracy: 0.3265\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 292s 6s/step - loss: 1.8151 - accuracy: 0.3484 - val_loss: 1.7722 - val_accuracy: 0.3807\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 310s 6s/step - loss: 1.6268 - accuracy: 0.4273 - val_loss: 1.6453 - val_accuracy: 0.4203\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 316s 6s/step - loss: 1.5994 - accuracy: 0.4531 - val_loss: 1.4452 - val_accuracy: 0.4861\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 312s 6s/step - loss: 1.3902 - accuracy: 0.5174 - val_loss: 1.3473 - val_accuracy: 0.5610\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 311s 6s/step - loss: 1.2110 - accuracy: 0.5818 - val_loss: 1.2168 - val_accuracy: 0.5688\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 308s 6s/step - loss: 0.9653 - accuracy: 0.6654 - val_loss: 0.9317 - val_accuracy: 0.6810\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 315s 6s/step - loss: 0.8218 - accuracy: 0.7085 - val_loss: 0.8594 - val_accuracy: 0.7219\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 313s 6s/step - loss: 0.7273 - accuracy: 0.7395 - val_loss: 1.1220 - val_accuracy: 0.6848\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 333s 7s/step - loss: 0.6750 - accuracy: 0.7651 - val_loss: 0.6370 - val_accuracy: 0.7858\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 282s 6s/step - loss: 0.5482 - accuracy: 0.8072 - val_loss: 0.6988 - val_accuracy: 0.7901\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 317s 6s/step - loss: 0.6220 - accuracy: 0.7851 - val_loss: 0.5761 - val_accuracy: 0.8064\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 320s 6s/step - loss: 0.9032 - accuracy: 0.7217 - val_loss: 1.2370 - val_accuracy: 0.5603\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 318s 6s/step - loss: 0.9307 - accuracy: 0.6684 - val_loss: 0.7407 - val_accuracy: 0.7381\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 314s 6s/step - loss: 0.7003 - accuracy: 0.7573 - val_loss: 0.8248 - val_accuracy: 0.7437\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 326s 6s/step - loss: 0.6051 - accuracy: 0.7841 - val_loss: 0.7476 - val_accuracy: 0.7627\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 325s 6s/step - loss: 0.4248 - accuracy: 0.8494 - val_loss: 1.1075 - val_accuracy: 0.7562\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 323s 6s/step - loss: 0.4632 - accuracy: 0.8386 - val_loss: 0.6261 - val_accuracy: 0.8191\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 311s 6s/step - loss: 0.3968 - accuracy: 0.8577 - val_loss: 0.9364 - val_accuracy: 0.7568\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 309s 6s/step - loss: 0.3643 - accuracy: 0.8730 - val_loss: 0.6530 - val_accuracy: 0.8195\n"
     ]
    }
   ],
   "source": [
    "# Using checkpoints to save models\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint('model-{epoch:03d}.model', monitor='val_loss', verbose=0,\n",
    "#                                                 save_best_only=True, mode='auto')\n",
    "\n",
    "# Fitting the model on the data. Saved the model in a variable to compare results\n",
    "history = model.fit(train_generator, epochs=20, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
